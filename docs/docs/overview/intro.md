---
sidebar_position: 1
title: ‚ú® Introduction
description: xTuring is an open-source AI personalization library.
slug: /
---

# xTuring

<!-- **xTuring** is an open-source AI personalization software. xTuring makes it easy to build and control
LLMs by providing simple interface to personalizing LLMs to your own data and application.

xTuring gives you the tools to:
- fine-tune LLMs using different approaches
- generate datasets from your data sources
- evaluate modified models

xTuring prioritizes:
- simplicity and productivity
- efficiency of compute and memory
- agility and customizability

### Installation
```bash
pip install xturing
```
-->


**Welcome to xTuring: Personalize AI your way**

In the world of AI, personalization is incredibly important for making AI truly powerful. This is where xTuring comes in ‚Äì it's a special open-source software that helps you make AI models, called Large Language Models (LLMs), work exactly the way you want them to.

What's great about xTuring is that it's super easy to use. It has a simple interface that's designed to help you customize LLMs for your specific needs, whether it's for your own data or applications. Basically, xTuring gives you complete control over personalizing AI, making it work just the way you need it to.

## The xTuring advantage

At xTuring, we have three important principles that guide everything we do:

1. **Simplicity and Productivity**: We want to make AI tasks easy to understand and do. Whether you're new to AI or an experienced developer, xTuring is designed to be simple and user-friendly. It helps you get things done efficiently.

2. **Efficiency of Compute and Memory**: We know that time and resources are precious. xTuring is built to make the most of your computer's power and memory. This means your AI projects will run smoothly and won't use up too much of your computer's resources.

3. **Agility and Customizability**: AI is always changing and evolving. That's why xTuring allows you to change and customize AI models to fit your needs. This helps you stay flexible and creative in the world of AI.

When you use xTuring, think of it as your personal AI workshop. You can make AI models better or create special AI tools for different jobs. xTuring is here to work with you.

So, if you're ready to add a personal touch to AI, welcome to xTuring. It's a journey filled with innovation, simplicity, and endless possibilities. Let's unlock the true power of AI, just the way you want it.

To get started with xTuring, check out the [Quickstart](/overview/quickstart) guide or try some of the examples below.

## Model examples

| Model | Examples |
| --- | --- |
| Bloom | [Bloom fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/bloom) |
| Cerebras-GPT | [Cerebras-GPT fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/cerebras) |
| Falcon | [Falcon 7B fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/falcon) |
| Galactica | [Galactica fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/galactica) |
| Generic Wrapper | [Any large language model fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/generic) |
| GPT-J | [GPT-J 6B LoRA fine-tuning with/without INT8 ](https://github.com/stochasticai/xturing/tree/main/examples/models/gptj) |
| GPT-2 | [GPT-2 fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/gpt2) |
| LLaMA | [LLaMA 7B fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/llama) |
| LLaMA 2 | [LLaMA 2 7B fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/llama2) |
| OPT | [OPT fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/models/opt) |

xTuring is licensed under [Apache 2.0](https://github.com/stochasticai/xturing/blob/main/LICENSE)

## Support
üí¨ Join our [Discord community](https://discord.gg/YxHuQq8b) and chat with other community members about ideas.
üïäÔ∏è Follow us on Twitter
[@stochasticai](https://twitter.com/stochasticai).

## About the team

**Meet the Team Behind xTuring: Making AI Simple and Accessible**

The people who created xTuring come from a place called Stochastic, where lots of smart and creative minds work together. We all share a big idea: we want to change the way AI works so that everyone can use it.

**Experts Working Together**: Our team is made up of different kinds of experts, like researchers and engineers. Each person has special skills in things like machines that learn, computers, and using AI in real life. These skills are what make xTuring so amazing.

**Always Thinking of New Ideas**: We're always thinking about how to make AI easier to use and more helpful. Our team is spread out all over the world, but we work together really well. This means xTuring has lots of cool ideas from people everywhere.

**Keeping Things Simple**: Even though AI can be very complicated, we believe it should be easy for everyone to use. That's why xTuring is simple to understand and can do powerful things. We want everyone to be able to use AI and make it work for them.

**Here to Help You Succeed**: Our job doesn't stop with making xTuring. We're here to help you learn and use AI in the best way possible. We want you to feel confident using our tool in the fast-changing world of AI.

[Come Work with Us](/contributing) and be part of the future of AI with xTuring. We're all about new ideas and making AI better for everyone. We're here to help you every step of the way.
