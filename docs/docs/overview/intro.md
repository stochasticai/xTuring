---
sidebar_position: 1
title: âœ¨ Introduction
description: xTuring is an open-source AI personalization library.
slug: /
---

# xTuring

**xTuring** is an open-source AI personalization software. xTuring makes it easy to build and control
LLMs by providing simple interface to personalizing LLMs to your own data and application.

xTuring gives you the tools to:
- fine-tune LLMs using different approaches
- generate datasets from your data sources
- evaluate modified models

xTuring prioritizes:
- simplicity and productivity
- efficiency of compute and memory
- agility and customizability

### Installation
```bash
pip install xturing
```

You can quickly get started with xTuring by following the [Quickstart](/overview/quickstart) guide or use one of the examples below.

### UI Playground

![Playground UI Demo](/img/playground/ui-playground.gif)

### CLI Playground

![Playground CLI Demo](/img/playground/cli-playground.gif)

### Models supported

| Model | Examples |
| --- | --- |
| Bloom | [Bloom fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/bloom) |
| Cerebras-GPT | [Cerebras-GPT fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/cerebras) |
| Falcon | [Falcon 7B fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/falcon) |
| Galactica | [Galactica fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/galactica) |
| Generic Wrapper | [Any large language model fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/generic) |
| GPT-J | [GPT-J 6B LoRA fine-tuning with/without INT8 ](https://github.com/stochasticai/xturing/tree/main/examples/gptj) |
| GPT-2 | [GPT-2 fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/gpt2) |
| LLaMA | [LLaMA 7B fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/llama) |
| LLaMA 2 | [LLaMA 2 7B fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/llama2) |
| OPT | [OPT fine-tuning on Alpaca dataset with/without LoRA and with/without INT8](https://github.com/stochasticai/xturing/tree/main/examples/opt) |

xTuring is licensed under [Apache 2.0](https://github.com/stochasticai/xturing/blob/main/LICENSE)

### Support
[ðŸ’¬ Join Community Discord](https://discord.gg/YxHuQq8b) <br/>
[@stochasticai](https://twitter.com/stochasticai)
