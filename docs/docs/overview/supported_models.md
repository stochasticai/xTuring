---
sidebar_position: 4
title: ðŸ¦¾ Supported models
description: Models Supported by xTuring
---

<!-- # Models supported by xTuring -->
## Base versions
|   Model | Model Key | LoRA | INT8 | LoRA + INT8 | LoRA + INT4 |
| ------ | --- | :---: | :---: | :---: | :---: |
| BLOOM 1.1B| bloom |  âœ… | âœ… | âœ… | âœ… |
| Cerebras 1.3B| cerebras | âœ…  | âœ… | âœ… | âœ… |
| DistilGPT-2 | distilgpt2 | âœ…  | âœ… | âœ… | âœ… |
| Falcon 7B | falcon | âœ…  | âœ… | âœ… | âœ… |
| Galactica 6.7B| galactica | âœ…  | âœ… | âœ… | âœ… |
| GPT-J  6B | gptj | âœ… | âœ… | âœ… | âœ… |
| GPT-2  | gpt2 | âœ…  | âœ… | âœ… | âœ… |
| LLaMA  7B | llama | âœ… | âœ… | âœ… | âœ… |
| LLaMA2  | llama2 | âœ… | âœ… | âœ… | âœ… |
| MiniMaxM2 | minimax_m2 | âœ… | âœ… | âœ… | âœ… |
| Qwen3 0.6B | qwen3_0_6b | âœ… | âœ… | âœ… | âœ… |
| OPT 1.3B  | opt | âœ… | âœ… |  âœ… | âœ… |

### Memory-efficient versions
> The above mentioned are the base variants of the LLMs. Below are the templates to get their `LoRA`, `INT8`, `INT8 + LoRA` and `INT4 + LoRA` versions.

| Version | Template |
| -- | -- |
| LoRA |  <model_key>_lora|
| INT8 |  <model_key>_int8|
| INT8 + LoRA |  <model_key>_lora_int8|

### INT4 Precision model versions
> In order to load any model's __`INT4+LoRA`__ version, you will need to make use of `GenericLoraKbitModel` class from `xturing.models`. Below is how to use it:
```python
from xturing.models import GenericLoraKbitModel
model = GenericLoraKbitModel('/path/to/model')
```
The `/path/to/model` can be replaced with you local directory or any HuggingFace library model like `facebook/opt-1.3b`.
