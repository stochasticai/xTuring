defaults:
  learning_rate: 1e-5
  gradient_accumulation_steps: 1
  batch_size: 1
  weight_decay: 0.00
  warmup_steps: 50
  eval_steps: 5000
  save_steps: 5000
  max_length: 512
  num_train_epochs: 1
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  optimizer_name: adamw
  output_dir: saved_model

llama:
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3
  optimizer_name: cpu_adam

llama_lora:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 1

llama_lora_int8:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8
  max_length: 256

gptj:
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3
  optimizer_name: cpu_adam

gptj_lora:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 1

gptj_lora_int8:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8
  max_length: 256

gpt2:
  learning_rate: 1e-3
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8

gpt2_lora:
  learning_rate: 3e-3
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 16

gpt2_lora_int8:
  learning_rate: 3e-3
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 16

distilgpt2:
  learning_rate: 1e-3
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8

distilgpt2_lora:
  learning_rate: 3e-3
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 16

galactica:
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3
  optimizer_name: cpu_adam

galactica_lora:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 1

galactica_lora_int8:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8
  max_length: 256

opt:
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3

opt_lora:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 1

opt_lora_int8:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8
  max_length: 256

cerebras:
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3

cerebras_lora:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 4

cerebras_lora_int8:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8
  max_length: 256

bloom:
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3

bloom_lora:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 4

bloom_lora_int8:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 3
  batch_size: 8
  max_length: 256
